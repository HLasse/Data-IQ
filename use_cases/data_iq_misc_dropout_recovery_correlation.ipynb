{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4ER17PSHfGat"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import xgboost as xgb\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_iq.dataiq_class import *\n",
    "from src.models.neuralnets import *\n",
    "from src.utils.data_loader import *\n",
    "from src.utils.utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xi_DdYaSq1DG"
   },
   "outputs": [],
   "source": [
    "dataset = \"support\"\n",
    "(\n",
    "    train_loader,\n",
    "    train_data,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    X_train_pd,\n",
    "    y_train_pd,\n",
    "    X_test_pd,\n",
    "    y_test_pd,\n",
    "    nlabels,\n",
    "    corr_vals,\n",
    "    column_ids,\n",
    "    df,\n",
    ") = load_dataset(dataset)\n",
    "\n",
    "n_feats = X_train.shape[1]\n",
    "nlabels = len(np.unique(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-n2FBcOxVVGO"
   },
   "outputs": [],
   "source": [
    "# Add a neural net with dropout to assess the effect of additional randomness\n",
    "class Net6(nn.Module):\n",
    "    def __init__(self, input_size=12, num_units=64, nonlin=F.relu, nlabels=2, p=0.1):\n",
    "        super(Net6, self).__init__()\n",
    "\n",
    "        self.dense0 = nn.Linear(input_size, num_units)\n",
    "        self.dense1 = nn.Linear(num_units, 32)\n",
    "        self.dense2 = nn.Linear(32, 16)\n",
    "        self.dense3 = nn.Linear(16, 8)\n",
    "        self.nonlin = nonlin\n",
    "        self.output = nn.Linear(8, nlabels)\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = F.relu(self.dense1(X))\n",
    "        X = self.dropout(X)\n",
    "        X = F.relu(self.dense2(X))\n",
    "        X = self.dropout(X)\n",
    "        X = F.relu(self.dense3(X))\n",
    "        X = self.dropout(X)\n",
    "        X = F.softmax(self.output(X))\n",
    "        # X = self.output(X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KqbpXIbeInMx",
    "outputId": "16158b66-81a1-4140-d0c1-0eb0f37167b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/models/neuralnets.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n",
      "/Users/nabeel/Documents/GitHub/Data-IQ/dataiq_env_test/lib/python3.7/site-packages/ipykernel_launcher.py:60: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/nabeel/Documents/GitHub/Data-IQ/dataiq_env_test/lib/python3.7/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.64153 | Acc: 0.687\n",
      "Epoch 002: | Loss: 0.60017 | Acc: 0.688\n",
      "Epoch 003: | Loss: 0.57989 | Acc: 0.687\n",
      "Epoch 004: | Loss: 0.56426 | Acc: 0.698\n",
      "Epoch 005: | Loss: 0.54475 | Acc: 0.755\n",
      "Epoch 006: | Loss: 0.53715 | Acc: 0.761\n",
      "Epoch 007: | Loss: 0.53060 | Acc: 0.772\n",
      "Epoch 008: | Loss: 0.52491 | Acc: 0.781\n",
      "Epoch 009: | Loss: 0.52163 | Acc: 0.785\n",
      "Epoch 010: | Loss: 0.51631 | Acc: 0.791\n",
      "Epoch 011: | Loss: 0.51294 | Acc: 0.797\n",
      "Epoch 012: | Loss: 0.50821 | Acc: 0.803\n",
      "Epoch 013: | Loss: 0.50437 | Acc: 0.808\n",
      "Epoch 014: | Loss: 0.50097 | Acc: 0.812\n",
      "Epoch 015: | Loss: 0.49691 | Acc: 0.817\n",
      "Epoch 016: | Loss: 0.49288 | Acc: 0.821\n",
      "Epoch 017: | Loss: 0.48943 | Acc: 0.825\n",
      "Epoch 018: | Loss: 0.48572 | Acc: 0.827\n",
      "Epoch 019: | Loss: 0.48193 | Acc: 0.832\n",
      "Epoch 020: | Loss: 0.47973 | Acc: 0.836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/models/neuralnets.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.62597 | Acc: 0.687\n",
      "Epoch 002: | Loss: 0.59529 | Acc: 0.687\n",
      "Epoch 003: | Loss: 0.57568 | Acc: 0.687\n",
      "Epoch 004: | Loss: 0.56590 | Acc: 0.688\n",
      "Epoch 005: | Loss: 0.55944 | Acc: 0.688\n",
      "Epoch 006: | Loss: 0.55476 | Acc: 0.687\n",
      "Epoch 007: | Loss: 0.54986 | Acc: 0.742\n",
      "Epoch 008: | Loss: 0.54573 | Acc: 0.769\n",
      "Epoch 009: | Loss: 0.54088 | Acc: 0.777\n",
      "Epoch 010: | Loss: 0.53645 | Acc: 0.784\n",
      "Epoch 011: | Loss: 0.53217 | Acc: 0.790\n",
      "Epoch 012: | Loss: 0.52762 | Acc: 0.795\n",
      "Epoch 013: | Loss: 0.52365 | Acc: 0.802\n",
      "Epoch 014: | Loss: 0.51926 | Acc: 0.810\n",
      "Epoch 015: | Loss: 0.51615 | Acc: 0.811\n",
      "Epoch 016: | Loss: 0.51123 | Acc: 0.818\n",
      "Epoch 017: | Loss: 0.50876 | Acc: 0.820\n",
      "Epoch 018: | Loss: 0.50574 | Acc: 0.821\n",
      "Epoch 019: | Loss: 0.50198 | Acc: 0.826\n",
      "Epoch 020: | Loss: 0.49930 | Acc: 0.829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/models/neuralnets.py:82: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.64771 | Acc: 0.687\n",
      "Epoch 002: | Loss: 0.61312 | Acc: 0.687\n",
      "Epoch 003: | Loss: 0.59322 | Acc: 0.687\n",
      "Epoch 004: | Loss: 0.57932 | Acc: 0.691\n",
      "Epoch 005: | Loss: 0.56672 | Acc: 0.732\n",
      "Epoch 006: | Loss: 0.55585 | Acc: 0.748\n",
      "Epoch 007: | Loss: 0.55028 | Acc: 0.750\n",
      "Epoch 008: | Loss: 0.54618 | Acc: 0.754\n",
      "Epoch 009: | Loss: 0.54335 | Acc: 0.757\n",
      "Epoch 010: | Loss: 0.54186 | Acc: 0.757\n",
      "Epoch 011: | Loss: 0.53992 | Acc: 0.758\n",
      "Epoch 012: | Loss: 0.53871 | Acc: 0.760\n",
      "Epoch 013: | Loss: 0.53698 | Acc: 0.762\n",
      "Epoch 014: | Loss: 0.53584 | Acc: 0.765\n",
      "Epoch 015: | Loss: 0.53402 | Acc: 0.767\n",
      "Epoch 016: | Loss: 0.53317 | Acc: 0.768\n",
      "Epoch 017: | Loss: 0.53236 | Acc: 0.770\n",
      "Epoch 018: | Loss: 0.53193 | Acc: 0.774\n",
      "Epoch 019: | Loss: 0.53027 | Acc: 0.774\n",
      "Epoch 020: | Loss: 0.52975 | Acc: 0.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/models/neuralnets.py:96: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.65064 | Acc: 0.659\n",
      "Epoch 002: | Loss: 0.61713 | Acc: 0.689\n",
      "Epoch 003: | Loss: 0.59915 | Acc: 0.689\n",
      "Epoch 004: | Loss: 0.58648 | Acc: 0.700\n",
      "Epoch 005: | Loss: 0.57504 | Acc: 0.724\n",
      "Epoch 006: | Loss: 0.56521 | Acc: 0.738\n",
      "Epoch 007: | Loss: 0.55788 | Acc: 0.743\n",
      "Epoch 008: | Loss: 0.55352 | Acc: 0.748\n",
      "Epoch 009: | Loss: 0.54963 | Acc: 0.751\n",
      "Epoch 010: | Loss: 0.54773 | Acc: 0.754\n",
      "Epoch 011: | Loss: 0.54535 | Acc: 0.755\n",
      "Epoch 012: | Loss: 0.54398 | Acc: 0.755\n",
      "Epoch 013: | Loss: 0.54285 | Acc: 0.755\n",
      "Epoch 014: | Loss: 0.54172 | Acc: 0.758\n",
      "Epoch 015: | Loss: 0.54033 | Acc: 0.759\n",
      "Epoch 016: | Loss: 0.53908 | Acc: 0.762\n",
      "Epoch 017: | Loss: 0.53798 | Acc: 0.762\n",
      "Epoch 018: | Loss: 0.53695 | Acc: 0.765\n",
      "Epoch 019: | Loss: 0.53618 | Acc: 0.765\n",
      "Epoch 020: | Loss: 0.53543 | Acc: 0.767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nabeel/Documents/GitHub/Data-IQ/dataiq_env_test/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.66280 | Acc: 0.664\n",
      "Epoch 002: | Loss: 0.60170 | Acc: 0.687\n",
      "Epoch 003: | Loss: 0.57969 | Acc: 0.688\n",
      "Epoch 004: | Loss: 0.56481 | Acc: 0.721\n",
      "Epoch 005: | Loss: 0.55563 | Acc: 0.748\n",
      "Epoch 006: | Loss: 0.54851 | Acc: 0.755\n",
      "Epoch 007: | Loss: 0.54053 | Acc: 0.761\n",
      "Epoch 008: | Loss: 0.53642 | Acc: 0.766\n",
      "Epoch 009: | Loss: 0.53226 | Acc: 0.769\n",
      "Epoch 010: | Loss: 0.52864 | Acc: 0.778\n",
      "Epoch 011: | Loss: 0.52531 | Acc: 0.783\n",
      "Epoch 012: | Loss: 0.52088 | Acc: 0.787\n",
      "Epoch 013: | Loss: 0.51787 | Acc: 0.792\n",
      "Epoch 014: | Loss: 0.51494 | Acc: 0.795\n",
      "Epoch 015: | Loss: 0.51105 | Acc: 0.800\n",
      "Epoch 016: | Loss: 0.50955 | Acc: 0.802\n",
      "Epoch 017: | Loss: 0.50617 | Acc: 0.808\n",
      "Epoch 018: | Loss: 0.50239 | Acc: 0.811\n",
      "Epoch 019: | Loss: 0.50074 | Acc: 0.812\n",
      "Epoch 020: | Loss: 0.49761 | Acc: 0.816\n",
      "Epoch 001: | Loss: 0.62782 | Acc: 0.687\n",
      "Epoch 002: | Loss: 0.59967 | Acc: 0.687\n",
      "Epoch 003: | Loss: 0.58064 | Acc: 0.687\n",
      "Epoch 004: | Loss: 0.56531 | Acc: 0.704\n",
      "Epoch 005: | Loss: 0.54789 | Acc: 0.750\n",
      "Epoch 006: | Loss: 0.54210 | Acc: 0.759\n",
      "Epoch 007: | Loss: 0.53697 | Acc: 0.769\n",
      "Epoch 008: | Loss: 0.53423 | Acc: 0.771\n",
      "Epoch 009: | Loss: 0.52677 | Acc: 0.779\n",
      "Epoch 010: | Loss: 0.52438 | Acc: 0.783\n",
      "Epoch 011: | Loss: 0.52203 | Acc: 0.786\n",
      "Epoch 012: | Loss: 0.51982 | Acc: 0.789\n",
      "Epoch 013: | Loss: 0.51721 | Acc: 0.790\n",
      "Epoch 014: | Loss: 0.51259 | Acc: 0.801\n",
      "Epoch 015: | Loss: 0.50853 | Acc: 0.803\n",
      "Epoch 016: | Loss: 0.50681 | Acc: 0.805\n",
      "Epoch 017: | Loss: 0.50163 | Acc: 0.810\n",
      "Epoch 018: | Loss: 0.50434 | Acc: 0.807\n",
      "Epoch 019: | Loss: 0.50198 | Acc: 0.809\n",
      "Epoch 020: | Loss: 0.50043 | Acc: 0.810\n",
      "Epoch 001: | Loss: 0.65862 | Acc: 0.688\n",
      "Epoch 002: | Loss: 0.62713 | Acc: 0.687\n",
      "Epoch 003: | Loss: 0.59438 | Acc: 0.687\n",
      "Epoch 004: | Loss: 0.57801 | Acc: 0.687\n",
      "Epoch 005: | Loss: 0.57055 | Acc: 0.687\n",
      "Epoch 006: | Loss: 0.56479 | Acc: 0.688\n",
      "Epoch 007: | Loss: 0.56219 | Acc: 0.687\n",
      "Epoch 008: | Loss: 0.55800 | Acc: 0.687\n",
      "Epoch 009: | Loss: 0.55624 | Acc: 0.686\n",
      "Epoch 010: | Loss: 0.55399 | Acc: 0.688\n",
      "Epoch 011: | Loss: 0.54978 | Acc: 0.695\n",
      "Epoch 012: | Loss: 0.54649 | Acc: 0.767\n",
      "Epoch 013: | Loss: 0.54237 | Acc: 0.775\n",
      "Epoch 014: | Loss: 0.53760 | Acc: 0.783\n",
      "Epoch 015: | Loss: 0.53713 | Acc: 0.780\n",
      "Epoch 016: | Loss: 0.53225 | Acc: 0.788\n",
      "Epoch 017: | Loss: 0.53127 | Acc: 0.790\n",
      "Epoch 018: | Loss: 0.53012 | Acc: 0.787\n",
      "Epoch 019: | Loss: 0.52658 | Acc: 0.793\n",
      "Epoch 020: | Loss: 0.52239 | Acc: 0.798\n",
      "Epoch 001: | Loss: 0.64404 | Acc: 0.688\n",
      "Epoch 002: | Loss: 0.61938 | Acc: 0.688\n",
      "Epoch 003: | Loss: 0.59600 | Acc: 0.687\n",
      "Epoch 004: | Loss: 0.58062 | Acc: 0.687\n",
      "Epoch 005: | Loss: 0.56826 | Acc: 0.696\n",
      "Epoch 006: | Loss: 0.55948 | Acc: 0.720\n",
      "Epoch 007: | Loss: 0.55276 | Acc: 0.730\n",
      "Epoch 008: | Loss: 0.54727 | Acc: 0.739\n",
      "Epoch 009: | Loss: 0.54266 | Acc: 0.745\n",
      "Epoch 010: | Loss: 0.54173 | Acc: 0.744\n",
      "Epoch 011: | Loss: 0.54000 | Acc: 0.751\n",
      "Epoch 012: | Loss: 0.53388 | Acc: 0.762\n",
      "Epoch 013: | Loss: 0.53246 | Acc: 0.762\n",
      "Epoch 014: | Loss: 0.53091 | Acc: 0.767\n",
      "Epoch 015: | Loss: 0.52793 | Acc: 0.763\n",
      "Epoch 016: | Loss: 0.52715 | Acc: 0.769\n",
      "Epoch 017: | Loss: 0.52198 | Acc: 0.775\n",
      "Epoch 018: | Loss: 0.52034 | Acc: 0.777\n",
      "Epoch 019: | Loss: 0.51921 | Acc: 0.781\n",
      "Epoch 020: | Loss: 0.51602 | Acc: 0.782\n",
      "Epoch 001: | Loss: 0.65594 | Acc: 0.677\n",
      "Epoch 002: | Loss: 0.62636 | Acc: 0.687\n",
      "Epoch 003: | Loss: 0.60979 | Acc: 0.687\n",
      "Epoch 004: | Loss: 0.59085 | Acc: 0.689\n",
      "Epoch 005: | Loss: 0.57830 | Acc: 0.707\n",
      "Epoch 006: | Loss: 0.56612 | Acc: 0.719\n",
      "Epoch 007: | Loss: 0.56228 | Acc: 0.719\n",
      "Epoch 008: | Loss: 0.55628 | Acc: 0.731\n",
      "Epoch 009: | Loss: 0.55552 | Acc: 0.733\n",
      "Epoch 010: | Loss: 0.54954 | Acc: 0.737\n",
      "Epoch 011: | Loss: 0.54621 | Acc: 0.742\n",
      "Epoch 012: | Loss: 0.54768 | Acc: 0.739\n",
      "Epoch 013: | Loss: 0.54200 | Acc: 0.751\n",
      "Epoch 014: | Loss: 0.54111 | Acc: 0.760\n",
      "Epoch 015: | Loss: 0.53761 | Acc: 0.762\n",
      "Epoch 016: | Loss: 0.53872 | Acc: 0.765\n",
      "Epoch 017: | Loss: 0.53349 | Acc: 0.769\n",
      "Epoch 018: | Loss: 0.53123 | Acc: 0.774\n",
      "Epoch 019: | Loss: 0.53096 | Acc: 0.775\n",
      "Epoch 020: | Loss: 0.53084 | Acc: 0.775\n"
     ]
    }
   ],
   "source": [
    "from aum import AUMCalculator\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 20\n",
    "\n",
    "# Instantiate NNs with different dropout probas\n",
    "MP1 = Net6(input_size=n_feats, nlabels=nlabels, p=0.1)\n",
    "MP2 = Net6(input_size=n_feats, nlabels=nlabels, p=0.2)\n",
    "MP3 = Net6(input_size=n_feats, nlabels=nlabels, p=0.3)\n",
    "MP4 = Net6(input_size=n_feats, nlabels=nlabels, p=0.4)\n",
    "MP5 = Net6(input_size=n_feats, nlabels=nlabels, p=0.5)\n",
    "\n",
    "nets = [\n",
    "    Net1(input_size=n_feats, nlabels=nlabels),\n",
    "    Net2(input_size=n_feats, nlabels=nlabels),\n",
    "    Net4(input_size=n_feats, nlabels=nlabels),\n",
    "    Net5(input_size=n_feats, nlabels=nlabels),\n",
    "    MP1,\n",
    "    MP2,\n",
    "    MP3,\n",
    "    MP4,\n",
    "    MP5,\n",
    "]\n",
    "\n",
    "checkpoint_list = []\n",
    "dataiq_list = []\n",
    "\n",
    "for i in range(len(nets)):\n",
    "\n",
    "    from aum import DatasetWithIndex\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=DatasetWithIndex(train_data), batch_size=128, shuffle=True\n",
    "    )\n",
    "    ckpt_nets = []\n",
    "    net_idx = i\n",
    "\n",
    "    net = nets[net_idx]\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    dataiq = DataIQ_Torch(X=X_train, y=y_train, sparse_labels=True)\n",
    "\n",
    "    for e in range(1, EPOCHS + 1):\n",
    "        net.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for X_batch, y_batch, sample_ids in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            sf = nn.LogSoftmax()\n",
    "            y_pred = net(X_batch)\n",
    "\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "\n",
    "            y_batch = y_batch.to(torch.int64)\n",
    "\n",
    "            loss = criterion(sf(y_pred), y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += (predicted == y_batch).sum().item() / len(y_batch)\n",
    "\n",
    "        dataiq.on_epoch_end(net, device=device)\n",
    "        print(\n",
    "            f\"Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}\"\n",
    "        )\n",
    "        ckpt_nets.append(deepcopy(net))\n",
    "\n",
    "    checkpoint_list.append(ckpt_nets)\n",
    "    dataiq_list.append(dataiq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZbWAL-_v373v",
    "outputId": "613cac9b-6374-4232-a389-8cecae333ade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Recovery Accuracy Across different parameterizations\n",
      "0.9180547928488619 0.014409336162798422\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# ACCURACY OF GROUP recovery\n",
    "\n",
    "percentile_thresh = 50\n",
    "conf_thresh = 0.5\n",
    "conf_thresh_low = 0.25\n",
    "conf_thresh_high = 0.75\n",
    "\n",
    "accs = []\n",
    "combos = [\n",
    "    (0, 1),\n",
    "    (0, 2),\n",
    "    (0, 3),\n",
    "    (0, 4),\n",
    "    (1, 2),\n",
    "    (1, 3),\n",
    "    (1, 4),\n",
    "    (2, 3),\n",
    "    (2, 4),\n",
    "    (3, 4),\n",
    "]\n",
    "\n",
    "N = len(nets) - 5  # we don't consider the random dropout models\n",
    "combos = list(combinations(list(range(N)), 2))\n",
    "for combo in combos:\n",
    "\n",
    "    # ASSESS MODEL 1\n",
    "    d_idx = combo[0]\n",
    "    aleatoric_train = dataiq_list[d_idx].aleatoric\n",
    "    confidence_train = dataiq_list[d_idx].confidence\n",
    "    variability_train = dataiq_list[d_idx].variability\n",
    "\n",
    "    hard_train = np.where(\n",
    "        (confidence_train <= conf_thresh_low)\n",
    "        & (aleatoric_train <= np.percentile(aleatoric_train, percentile_thresh))\n",
    "    )[0]\n",
    "    easy_train = np.where(\n",
    "        (confidence_train >= conf_thresh_high)\n",
    "        & (aleatoric_train <= np.percentile(aleatoric_train, percentile_thresh))\n",
    "    )[0]\n",
    "\n",
    "    hard_easy = np.concatenate((hard_train, easy_train))\n",
    "    ambig_train = []\n",
    "    for id in range(len(confidence_train)):\n",
    "        if id not in hard_easy:\n",
    "            ambig_train.append(id)\n",
    "    ambig_train = np.array(ambig_train)\n",
    "\n",
    "    # ASSESS MODEL 2\n",
    "    d_idx = combo[1]\n",
    "    aleatoric_train = dataiq_list[d_idx].aleatoric\n",
    "    confidence_train = dataiq_list[d_idx].confidence\n",
    "    variability_train = dataiq_list[d_idx].variability\n",
    "\n",
    "    hard_train2 = np.where(\n",
    "        (confidence_train <= conf_thresh_low)\n",
    "        & (aleatoric_train <= np.percentile(aleatoric_train, percentile_thresh))\n",
    "    )[0]\n",
    "    easy_train2 = np.where(\n",
    "        (confidence_train >= conf_thresh_high)\n",
    "        & (aleatoric_train <= np.percentile(aleatoric_train, percentile_thresh))\n",
    "    )[0]\n",
    "\n",
    "    hard_easy = np.concatenate((hard_train2, easy_train2))\n",
    "    ambig_train2 = []\n",
    "    for id in range(len(confidence_train)):\n",
    "        if id not in hard_easy:\n",
    "            ambig_train2.append(id)\n",
    "    ambig_train2 = np.array(ambig_train2)\n",
    "\n",
    "    model1 = []\n",
    "    model2 = []\n",
    "    # log the group label for each model to compare\n",
    "    for i in range(len(X_train)):\n",
    "\n",
    "        if i in easy_train:\n",
    "            model1.append(0)\n",
    "        if i in ambig_train:\n",
    "            model1.append(1)\n",
    "        if i in hard_train:\n",
    "            model1.append(2)\n",
    "\n",
    "        if i in easy_train2:\n",
    "            model2.append(0)\n",
    "        if i in ambig_train2:\n",
    "            model2.append(1)\n",
    "        if i in hard_train2:\n",
    "            model2.append(2)\n",
    "\n",
    "    assert len(model1) == len(model2)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    accs.append(accuracy_score(model1, model2))\n",
    "\n",
    "\n",
    "print(\"Mean Recovery Accuracy Across different parameterizations\")\n",
    "print(np.mean(accs), np.std(accs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORR Data IQ: 0.9250572240706032, 0.016952658303863166\n",
      "CORR Data Maps: 0.8187666744175925, 0.10413920842061333\n"
     ]
    }
   ],
   "source": [
    "  # CORR Data IQ VS Data MAP\n",
    "  \n",
    "  accs = []\n",
    "  correlation_dataiq = []\n",
    "  correlation_datamaps = []\n",
    "\n",
    "  combos = [(0,1), (0,2), (0,3), (0,4), (1,2), (1,3), (1,4),  (2,3), (2,4), (3,4)]\n",
    "  N = len(nets)\n",
    "  combos = list(combinations(list(range(N)),2))\n",
    "  for combo in combos:\n",
    "\n",
    "    # ASSESS MODEL 1\n",
    "    d_idx=combo[0]\n",
    "    aleatoric_train = dataiq_list[d_idx].aleatoric\n",
    "    confidence_train = dataiq_list[d_idx].confidence\n",
    "    variability_train = dataiq_list[d_idx].variability\n",
    "\n",
    "    hard_train = np.where((confidence_train <= conf_thresh_low) & (aleatoric_train <= np.percentile(aleatoric_train,   percentile_thresh)))[0]\n",
    "    easy_train = np.where((confidence_train >= conf_thresh_high) & (aleatoric_train <= np.percentile(aleatoric_train,   percentile_thresh)))[0]\n",
    "\n",
    "    hard_easy = np.concatenate((hard_train,easy_train))\n",
    "    ambig_train = []\n",
    "    for id in range(len(confidence_train)):\n",
    "      if id not in hard_easy:\n",
    "        ambig_train.append(id)\n",
    "    ambig_train= np.array(ambig_train)\n",
    "\n",
    "\n",
    "    # ASSESS MODEL 2\n",
    "    d_idx=combo[1]\n",
    "    aleatoric_train = dataiq_list[d_idx].aleatoric\n",
    "    confidence_train = dataiq_list[d_idx].confidence\n",
    "    variability_train = dataiq_list[d_idx].variability\n",
    "\n",
    "    hard_train2 = np.where((confidence_train <= conf_thresh_low) & (aleatoric_train <= np.percentile(aleatoric_train,   percentile_thresh)))[0]\n",
    "    easy_train2 = np.where((confidence_train >= conf_thresh_high) & (aleatoric_train <= np.percentile(aleatoric_train,   percentile_thresh)))[0]\n",
    "\n",
    "    hard_easy = np.concatenate((hard_train2,easy_train2))\n",
    "    ambig_train2 = []\n",
    "    for id in range(len(confidence_train)):\n",
    "      if id not in hard_easy:\n",
    "        ambig_train2.append(id)\n",
    "    ambig_train2= np.array(ambig_train2)\n",
    "\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    x_rv1_dataiq = dataiq_list[combo[0]].aleatoric\n",
    "    x_rv2_dataiq = dataiq_list[combo[1]].aleatoric\n",
    "\n",
    "    x_rv1_datamaps = dataiq_list[combo[0]].variability\n",
    "    x_rv2_datamaps = dataiq_list[combo[1]].variability\n",
    "\n",
    "    y_rv1 = dataiq_list[combo[1]].confidence\n",
    "    y_rv2 = dataiq_list[combo[1]].confidence\n",
    "    \n",
    "    # correlation data-iq\n",
    "    corr_x_dataiq = np.corrcoef(x_rv1_dataiq,x_rv2_dataiq)[0,1]\n",
    "    # correlation data maps\n",
    "    corr_x_datamaps = np.corrcoef(x_rv1_datamaps,x_rv2_datamaps)[0,1]\n",
    "    # y-corr\n",
    "    corr_y = np.corrcoef(y_rv1, y_rv2)[0,1]\n",
    "    \n",
    "    corr_total_dataiq = (corr_x_dataiq+corr_y)/2\n",
    "    corr_total_datamaps = (corr_x_datamaps+corr_y)/2\n",
    "    \n",
    "    correlation_dataiq.append(corr_total_dataiq)\n",
    "    correlation_datamaps.append(corr_total_datamaps)\n",
    "\n",
    "\n",
    "print(f'CORR Data IQ: {np.mean(correlation_dataiq)}, {np.std(correlation_dataiq)}')\n",
    "print(f'CORR Data Maps: {np.mean(correlation_datamaps)}, {np.std(correlation_datamaps)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AzNLW1bKYdrk"
   },
   "outputs": [],
   "source": [
    "def get_even_numbers(numbers):\n",
    "    even_numbers = []\n",
    "\n",
    "    for number in numbers:\n",
    "        if number % 2 == 0:\n",
    "            even_numbers.append(number)\n",
    "\n",
    "    return even_numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GwrZGYjzQ6pt",
    "outputId": "536c4f92-f5b9-4a2d-b3e1-7648b4798ee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 CORR: 0.13070771265774966\n",
      "1 CORR: 0.17244050912559034\n",
      "2 CORR: 0.10489868149161338\n",
      "3 CORR: 0.07915564132854343\n",
      " Mean correlation of weights: 0.1218006361508742, 0.03445288232316364\n"
     ]
    }
   ],
   "source": [
    "##### Assess weight CORRELATION - do it based on cosine similarity like Jin et al.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "auto = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (7, 8), (8, 9)]\n",
    "corr_scores = []\n",
    "\n",
    "N = len(nets) - 5\n",
    "\n",
    "\n",
    "for modelnum in range(N):\n",
    "\n",
    "    params = []\n",
    "    for i in range(len(checkpoint_list[modelnum])):\n",
    "        layer_params = []\n",
    "        mymodel = checkpoint_list[modelnum][i]\n",
    "\n",
    "        for name, param in mymodel.named_parameters():\n",
    "            layer_params.append(param.cpu().detach().numpy())\n",
    "\n",
    "        params.append(layer_params)\n",
    "\n",
    "    correlations = []\n",
    "\n",
    "    for idx in auto:\n",
    "        m1 = idx[0]\n",
    "        m2 = idx[1]\n",
    "\n",
    "        sum = 0\n",
    "\n",
    "        param_len = list(range(len(params[0])))\n",
    "\n",
    "        weights = get_even_numbers(param_len)\n",
    "        biases = [1, 3, 5, 7, 9]\n",
    "\n",
    "        for i in weights:\n",
    "            corrcoef = cosine_similarity(params[m1][i], params[m2][i])\n",
    "            sum += np.mean(corrcoef)\n",
    "\n",
    "        correlations.append(sum / 5)\n",
    "\n",
    "    print(f\"{modelnum} CORR: {np.mean(correlations)}\")\n",
    "    corr_scores.append(np.mean(correlations))\n",
    "\n",
    "print(f\" Mean correlation of weights: {np.mean(corr_scores)}, {np.std(corr_scores)}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dataiq_env_test",
   "language": "python",
   "name": "dataiq_env_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
