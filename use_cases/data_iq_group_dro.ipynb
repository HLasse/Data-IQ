{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wTKrKic_6XSY"
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "import random, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from copy import deepcopy\n",
    "from random import sample\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_iq.dataiq_class import *\n",
    "from src.utils.utils import *\n",
    "from src.models.neuralnets import *\n",
    "from src.utils.data_loader import *\n",
    "from src.utils.group_dro_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "hWAaV8qyeDfs"
   },
   "outputs": [],
   "source": [
    "dataset = 'covid'\n",
    "train_loader, train_data, X_train, y_train, X_test, y_test, X_train_pd, y_train_pd, X_test_pd, y_test_pd, nlabels, corr_vals, column_ids, df = load_dataset(dataset)\n",
    "\n",
    "try:\n",
    "  X_test = X_test.to_numpy()\n",
    "except:\n",
    "  pass\n",
    "\n",
    "try:\n",
    "  y_test = y_test.values\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbPRtHx3-QRO"
   },
   "source": [
    "# TRAIN BASELINE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TB6PtBPeFF1M",
    "outputId": "c660c142-9d9f-497f-825d-b82efbc2b545"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/src/models/neuralnets.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.68592 | Acc: 0.553\n",
      "Epoch 002: | Loss: 0.67793 | Acc: 0.544\n",
      "Epoch 003: | Loss: 0.65535 | Acc: 0.650\n",
      "Epoch 004: | Loss: 0.59964 | Acc: 0.698\n",
      "Epoch 005: | Loss: 0.57595 | Acc: 0.726\n",
      "Epoch 006: | Loss: 0.57389 | Acc: 0.723\n",
      "Epoch 007: | Loss: 0.56123 | Acc: 0.738\n",
      "Epoch 008: | Loss: 0.56353 | Acc: 0.735\n",
      "Epoch 009: | Loss: 0.56534 | Acc: 0.733\n",
      "Epoch 010: | Loss: 0.56427 | Acc: 0.739\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "EPOCHS=10\n",
    "BATCH_SIZE=128\n",
    "\n",
    "latent_test=True\n",
    "nlabels = len(np.unique(y_train))\n",
    "\n",
    "if dataset=='fetal':\n",
    "  EPOCHS_FETAL=20\n",
    "  EPOCHS=EPOCHS_FETAL\n",
    "\n",
    "n_feats = X_train.shape[1]\n",
    "train_data = TrainData(torch.FloatTensor(X_train), \n",
    "                      torch.FloatTensor(y_train))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "loss_list=[]\n",
    "loss_list_test = []\n",
    "checkpoint_list = []\n",
    "dataiq_list=[]\n",
    "\n",
    "for i in range(1):\n",
    "  from aum import DatasetWithIndex\n",
    "  train_loader = DataLoader(dataset=DatasetWithIndex(train_data), batch_size=BATCH_SIZE, shuffle=True)\n",
    "  ckpt_nets = []\n",
    "  net = Net1(input_size=n_feats,nlabels=nlabels)\n",
    "  net.to(device)\n",
    "  criterion = torch.nn.NLLLoss()\n",
    "\n",
    "  optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "  dataiq = DataIQ_Torch(X=X_train , y=y_train, sparse_labels=True)\n",
    "\n",
    "  for e in range(1, EPOCHS+1):\n",
    "      net.train()\n",
    "      epoch_loss = 0\n",
    "      epoch_acc = 0\n",
    "      for X_batch, y_batch, sample_ids in train_loader:\n",
    "          X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "          optimizer.zero_grad()\n",
    "          sf = nn.LogSoftmax()\n",
    "          y_pred = net(X_batch)\n",
    "\n",
    "          _, predicted = torch.max(y_pred.data, 1)\n",
    "\n",
    "          y_batch=y_batch.to(torch.int64)\n",
    "          \n",
    "          loss = criterion(sf(y_pred), y_batch)\n",
    "\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          \n",
    "          epoch_loss += loss.item()\n",
    "          epoch_acc += (predicted == y_batch).sum().item()/len(y_batch)\n",
    "\n",
    "      \n",
    "      dataiq.on_epoch_end(net, device=device)\n",
    "      loss_list.append(epoch_loss/len(train_loader))\n",
    "      print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "      ckpt_nets.append(deepcopy(net))\n",
    "\n",
    "  checkpoint_list.append(ckpt_nets)\n",
    "  dataiq_list.append(dataiq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPzvacOK-TLN"
   },
   "source": [
    "# GET INTERMEDIATE (LATENT) REPRESENTATIONS WHICH WE WILL PROJECT WITH UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "aQmLc21t7Xus"
   },
   "outputs": [],
   "source": [
    "mymodel=checkpoint_list[0][9]\n",
    "\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "mymodel.dense3.register_forward_hook(get_activation('dense3'))\n",
    "output = mymodel(torch.tensor(X_train,device=device))\n",
    "intermediate_train = activation['dense3'].cpu().numpy()\n",
    "\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "mymodel.dense3.register_forward_hook(get_activation('dense3'))\n",
    "output = mymodel(torch.tensor(X_test,device=device).float())\n",
    "intermediate_test = activation['dense3'].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWOTeH1j-b8F"
   },
   "source": [
    "# UMAP TRANSFORMATION - LOWER DIMENSIONAL REPRESENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "PZ_Lp6Al7gFA"
   },
   "outputs": [],
   "source": [
    "mapper = umap.UMAP().fit(intermediate_train, y=y_train)\n",
    "embedding_train = mapper.transform(intermediate_train)\n",
    "embedding_test = mapper.transform(intermediate_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bi_0MXeF-hcr"
   },
   "source": [
    "# GET DATA-IQ subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jM94R9byMCuG",
    "outputId": "4c0f9f75-c611-4c80-87fa-e4b71d4a4eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 341 2064 1724\n"
     ]
    }
   ],
   "source": [
    "d_idx=0\n",
    "aleatoric_train = dataiq_list[d_idx].aleatoric\n",
    "confidence_train = dataiq_list[d_idx].confidence\n",
    "\n",
    "percentile_thresh = 50\n",
    "conf_thresh = 0.5\n",
    "conf_thresh_low = 0.25\n",
    "conf_thresh_high = 0.75\n",
    "\n",
    "hard_train = np.where((confidence_train <= conf_thresh_low) & (aleatoric_train <= np.percentile(aleatoric_train,   percentile_thresh)))[0]\n",
    "easy_train = np.where((confidence_train >= conf_thresh_high) & (aleatoric_train <= np.percentile(aleatoric_train,   percentile_thresh)))[0]\n",
    "\n",
    "hard_easy = np.concatenate((hard_train,easy_train))\n",
    "ambig_train = []\n",
    "for id in range(len(confidence_train)):\n",
    "  if id not in hard_easy:\n",
    "    ambig_train.append(id)\n",
    "ambig_train= np.array(ambig_train)\n",
    "\n",
    "print('Train :', len(hard_train),  len(ambig_train),  len(easy_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgfUARyF-kgB"
   },
   "source": [
    "# Now use neighbors in the UMAP to get potential test group labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3o4It84x-Hoi",
    "outputId": "2d528230-ccda-4266-b1ba-dcc2615e01df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test : 92 1424 1237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y_train_groups = []\n",
    "for i in range(len(y_train)):\n",
    "  if i in easy_train:\n",
    "    y_train_groups.append(0)\n",
    "  elif i in ambig_train:\n",
    "    y_train_groups.append(1)\n",
    "  elif i in hard_train:\n",
    "    y_train_groups.append(2)\n",
    "y_train_groups = np.array(y_train_groups)\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(X= embedding_train, y=y_train_groups)\n",
    "test_groups = neigh.predict(embedding_test)\n",
    "\n",
    "if latent_test:\n",
    "  easy_test = np.where(test_groups==0)[0]\n",
    "  ambig_test = np.where(test_groups==1)[0]\n",
    "  hard_test = np.where(test_groups==2)[0]\n",
    "print('Test :', len(hard_test),  len(ambig_test),  len(easy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QWpZ0fY-qGy"
   },
   "source": [
    "# GROUP-DRO: subgroups are clusters within the Data-IQ subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDZgOvNK-2g4"
   },
   "source": [
    "### Get clusters within the Data-IQ subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "B4Siwyv2LV7T"
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import metrics\n",
    "\n",
    "# Superclass: Data-IQ easy group\n",
    "partition0 = easy_train\n",
    "sil_score = []\n",
    "for i in range(10):\n",
    "  gm = GaussianMixture(n_components=i+2, random_state=0).fit(embedding_train[partition0,:])\n",
    "  clusters = gm.predict(embedding_train[partition0,:])\n",
    "  sil_score.append(metrics.silhouette_score(embedding_train[partition0,:], clusters, metric='euclidean'))\n",
    "best = np.argmax(sil_score)+2\n",
    "gm = GaussianMixture(n_components=best, random_state=0).fit(embedding_train[partition0,:])\n",
    "clusters0 = gm.predict(embedding_train[partition0,:])\n",
    "\n",
    "# Superclass: Data-IQ ambig group\n",
    "partition1 = ambig_train\n",
    "sil_score = []\n",
    "for i in range(10):\n",
    "  gm = GaussianMixture(n_components=i+2, random_state=0).fit(embedding_train[partition1,:])\n",
    "  clusters = gm.predict(embedding_train[partition1,:])\n",
    "  sil_score.append(metrics.silhouette_score(embedding_train[partition1,:], clusters, metric='euclidean'))\n",
    "best = np.argmax(sil_score)+2\n",
    "gm = GaussianMixture(n_components=best, random_state=0).fit(embedding_train[partition1,:])\n",
    "clusters1 = gm.predict(embedding_train[partition1,:])\n",
    "\n",
    "# Superclass: Data-IQ hard group\n",
    "partition2 = hard_train\n",
    "sil_score = []\n",
    "for i in range(10):\n",
    "  gm = GaussianMixture(n_components=i+2, random_state=0).fit(embedding_train[partition2,:])\n",
    "  clusters = gm.predict(embedding_train[partition2,:])\n",
    "  sil_score.append(metrics.silhouette_score(embedding_train[partition2,:], clusters, metric='euclidean'))\n",
    "best = np.argmax(sil_score)+2\n",
    "gm = GaussianMixture(n_components=best, random_state=0).fit(embedding_train[partition2,:])\n",
    "clusters2 = gm.predict(embedding_train[partition2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYHeAKG2-7fL"
   },
   "source": [
    "### Assign the subclass labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "EV8iFs_7t0CV"
   },
   "outputs": [],
   "source": [
    "X_trainG = np.concatenate((X_train[partition0,:],X_train[partition1,:],X_train[partition2,:]))\n",
    "y_trainG = np.concatenate((y_train[partition0],y_train[partition1],y_train[partition2]))\n",
    "superclass_labels = y_trainG\n",
    "\n",
    "max0 = np.max(np.unique(clusters0))\n",
    "max1 = np.max(np.unique(clusters1))\n",
    "clusters1_xp = clusters1+(max0+1)\n",
    "clusters2_xp = clusters2+(max0+max1+1+1)\n",
    "subclass_labels = np.concatenate((clusters0, clusters1_xp, clusters2_xp))\n",
    "\n",
    "class_map = {}\n",
    "superclass_set = sorted(set(np.array(superclass_labels)))\n",
    "for superclass in superclass_set:\n",
    "    class_map[superclass] = sorted(\n",
    "        np.unique(np.array(subclass_labels[superclass_labels == superclass])))\n",
    "sup_sub_map = class_map\n",
    "\n",
    "class_map = torch.tensor(subclass_labels) == torch.arange(len(np.unique(subclass_labels))).unsqueeze(1).long()\n",
    "subclass_counts = class_map.sum(1).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8vkJkLA_MVU"
   },
   "source": [
    "### Train w/ Group-DRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ehMIQaWxIlzx",
    "outputId": "8b60e729-1a98-406e-e6a4-b6fc2171a12a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/src/models/neuralnets.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n",
      "/content/src/utils/group_dro_helpers.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  co = criterion(sf(y_pred), y_batch, y_subclass)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.60756 | Acc: 0.623\n",
      "Epoch 002: | Loss: 0.58245 | Acc: 0.671\n",
      "Epoch 003: | Loss: 0.57454 | Acc: 0.705\n",
      "Epoch 004: | Loss: 0.56736 | Acc: 0.718\n",
      "Epoch 005: | Loss: 0.57973 | Acc: 0.718\n",
      "Epoch 006: | Loss: 0.57834 | Acc: 0.725\n",
      "Epoch 007: | Loss: 0.58031 | Acc: 0.727\n",
      "Epoch 008: | Loss: 0.58269 | Acc: 0.719\n",
      "Epoch 009: | Loss: 0.58639 | Acc: 0.725\n",
      "Epoch 010: | Loss: 0.57613 | Acc: 0.722\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "EPOCHS=10\n",
    "BATCH_SIZE=16\n",
    "num_subclasses = len(subclass_counts)\n",
    "\n",
    "if dataset=='fetal':\n",
    "  EPOCHS=EPOCHS_FETAL\n",
    "\n",
    "dro_netC = Net1(input_size=X_trainG.shape[1],nlabels=nlabels)\n",
    "dro_netC.to(device)\n",
    "\n",
    "train_data = TrainDataDRO(torch.FloatTensor(X_trainG), \n",
    "                       torch.FloatTensor(y_trainG), \n",
    "                       torch.tensor(subclass_labels))\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(dro_netC.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "robust=True\n",
    "\n",
    "if robust:\n",
    "    size_adjustment=1\n",
    "    size_adjustments = [size_adjustment] * num_subclasses\n",
    "else:\n",
    "    size_adjustments = None\n",
    "robust_lr=0.001\n",
    "criterion = LossComputer(criterion, robust, num_subclasses, subclass_counts,\n",
    "                          robust_lr, stable=True,\n",
    "                          size_adjustments=size_adjustments,\n",
    "                          auroc_version=False,\n",
    "                          class_map=sup_sub_map, use_cuda=True)\n",
    "\n",
    "dro_netC = train_loop(net=dro_netC, criterion=criterion, EPOCHS=EPOCHS, train_loader=train_loader,optimizer=optimizer, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpwcmAkR_Q6B"
   },
   "source": [
    "# GROUP-DRO: subgroups are clusters on the superclass (e.g George)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFmDAFtE_a2T"
   },
   "source": [
    "### Get clusters within the superclass labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "UoUQ5oNCNIis"
   },
   "outputs": [],
   "source": [
    "# Superclass: label=0\n",
    "partition0 = np.where(y_train==0)[0]\n",
    "sil_score = []\n",
    "for i in range(10):\n",
    "  gm = GaussianMixture(n_components=i+2, random_state=0).fit(embedding_train[partition0,:])\n",
    "  clusters = gm.predict(embedding_train[partition0,:])\n",
    "  sil_score.append(metrics.silhouette_score(embedding_train[partition0,:], clusters, metric='euclidean'))\n",
    "best = np.argmax(sil_score)+2\n",
    "gm = GaussianMixture(n_components=best, random_state=0).fit(embedding_train[partition0,:])\n",
    "clusters0 = gm.predict(embedding_train[partition0,:])\n",
    "\n",
    "# Superclass: label=1\n",
    "partition1 = np.where(y_train==1)[0]\n",
    "sil_score = []\n",
    "for i in range(10):\n",
    "  gm = GaussianMixture(n_components=i+2, random_state=0).fit(embedding_train[partition1,:])\n",
    "  clusters = gm.predict(embedding_train[partition1,:])\n",
    "  sil_score.append(metrics.silhouette_score(embedding_train[partition1,:], clusters, metric='euclidean'))\n",
    "best = np.argmax(sil_score)+2\n",
    "gm = GaussianMixture(n_components=best, random_state=0).fit(embedding_train[partition1,:])\n",
    "clusters1 = gm.predict(embedding_train[partition1,:])\n",
    "\n",
    "if dataset=='fetal':\n",
    "  # Superclass: label=2\n",
    "  partition2 = np.where(y_train==2)[0]\n",
    "  sil_score = []\n",
    "  for i in range(10):\n",
    "    gm = GaussianMixture(n_components=i+2, random_state=0).fit(embedding_train[partition2,:])\n",
    "    clusters = gm.predict(embedding_train[partition2,:])\n",
    "    sil_score.append(metrics.silhouette_score(embedding_train[partition2,:], clusters, metric='euclidean'))\n",
    "  best = np.argmax(sil_score)+2\n",
    "  gm = GaussianMixture(n_components=best, random_state=0).fit(embedding_train[partition2,:])\n",
    "  clusters2 = gm.predict(embedding_train[partition2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OU08XwC_dP1"
   },
   "source": [
    "### Assign the subclass labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "Om0RTvJkwc_d"
   },
   "outputs": [],
   "source": [
    "X_trainG = np.concatenate((X_train[partition0,:],X_train[partition1,:],X_train[partition2,:]))\n",
    "y_trainG = np.concatenate((y_train[partition0],y_train[partition1],y_train[partition2]))\n",
    "superclass_labels = y_trainG\n",
    "max0 = np.max(np.unique(clusters0))\n",
    "max1 = np.max(np.unique(clusters1))\n",
    "clusters1_xp = clusters1+(max0+1)\n",
    "clusters2_xp = clusters2+(max0+max1+1+1)\n",
    "subclass_labels = np.concatenate((clusters0, clusters1_xp, clusters2_xp))\n",
    "\n",
    "class_map = {}\n",
    "superclass_set = sorted(set(np.array(superclass_labels)))\n",
    "for superclass in superclass_set:\n",
    "    class_map[superclass] = sorted(\n",
    "        np.unique(np.array(subclass_labels[superclass_labels == superclass])))\n",
    "sup_sub_map = class_map\n",
    "\n",
    "class_map = torch.tensor(subclass_labels) == torch.arange(len(np.unique(subclass_labels))).unsqueeze(1).long()\n",
    "subclass_counts = class_map.sum(1).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGri78hL_pFR"
   },
   "source": [
    "### Train w/ Group-DRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4LKBaI-IINaA",
    "outputId": "1b63f64e-f231-43e7-f32f-e874a5c489ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/src/models/neuralnets.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n",
      "/content/src/utils/group_dro_helpers.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  co = criterion(sf(y_pred), y_batch, y_subclass)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.60536 | Acc: 0.504\n",
      "Epoch 002: | Loss: 0.59503 | Acc: 0.617\n",
      "Epoch 003: | Loss: 0.59392 | Acc: 0.632\n",
      "Epoch 004: | Loss: 0.59942 | Acc: 0.604\n",
      "Epoch 005: | Loss: 0.58561 | Acc: 0.643\n",
      "Epoch 006: | Loss: 0.59201 | Acc: 0.642\n",
      "Epoch 007: | Loss: 0.59323 | Acc: 0.652\n",
      "Epoch 008: | Loss: 0.60020 | Acc: 0.654\n",
      "Epoch 009: | Loss: 0.60596 | Acc: 0.652\n",
      "Epoch 010: | Loss: 0.60269 | Acc: 0.654\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "EPOCHS=10\n",
    "BATCH_SIZE=16\n",
    "num_subclasses = len(subclass_counts)\n",
    "if dataset=='fetal':\n",
    "  EPOCHS=EPOCHS_FETAL\n",
    "\n",
    "dro_netG = Net1(input_size=X_trainG.shape[1],nlabels=nlabels)\n",
    "dro_netG.to(device)\n",
    "\n",
    "train_data = TrainDataDRO(torch.FloatTensor(X_trainG), \n",
    "                       torch.FloatTensor(y_trainG), \n",
    "                       torch.tensor(subclass_labels))\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(dro_netG.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "robust=True\n",
    "\n",
    "if robust:\n",
    "    size_adjustment=1\n",
    "    size_adjustments = [size_adjustment] * num_subclasses\n",
    "else:\n",
    "    size_adjustments = None\n",
    "robust_lr=0.001\n",
    "criterion = LossComputer(criterion, robust, num_subclasses, subclass_counts,\n",
    "                          robust_lr, stable=True,\n",
    "                          size_adjustments=size_adjustments,\n",
    "                          auroc_version=False,\n",
    "                          class_map=sup_sub_map, use_cuda=True)\n",
    "\n",
    "dro_netG = train_loop(net=dro_netG, criterion=criterion, EPOCHS=EPOCHS, train_loader=train_loader,optimizer=optimizer, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMLmKsxg_uZb"
   },
   "source": [
    "# Group-DRO: on Data-IQ subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLi35eso_x8e"
   },
   "source": [
    "### Assign subclasses as Data-IQ subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "wxE5O9cCEq_z"
   },
   "outputs": [],
   "source": [
    "superclass_labels = y_train\n",
    "subclass_labels = []\n",
    "for i in range(len(y_train)):\n",
    "  if i in easy_train:\n",
    "    subclass_labels.append(0)\n",
    "  elif i in ambig_train:\n",
    "    subclass_labels.append(1)\n",
    "  else:\n",
    "    subclass_labels.append(2)\n",
    "subclass_labels = np.array(subclass_labels)\n",
    "\n",
    "class_map = {}\n",
    "superclass_set = sorted(set(np.array(superclass_labels)))\n",
    "for superclass in superclass_set:\n",
    "    class_map[superclass] = sorted(\n",
    "        np.unique(np.array(subclass_labels[superclass_labels == superclass])))\n",
    "sup_sub_map = class_map\n",
    "\n",
    "class_map = torch.tensor(subclass_labels) == torch.arange(3).unsqueeze(1).long()\n",
    "subclass_counts = class_map.sum(1).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5s2uh6f_0m9"
   },
   "source": [
    "### Train w/ Group-DRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nv36xUX0_2_l",
    "outputId": "fba58ccf-355b-41f6-e782-6d694b1ff213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.46626 | Acc: 0.548\n",
      "Epoch 002: | Loss: 0.37773 | Acc: 0.762\n",
      "Epoch 003: | Loss: 0.37450 | Acc: 0.787\n",
      "Epoch 004: | Loss: 0.38506 | Acc: 0.795\n",
      "Epoch 005: | Loss: 0.40209 | Acc: 0.793\n",
      "Epoch 006: | Loss: 0.41648 | Acc: 0.796\n",
      "Epoch 007: | Loss: 0.43462 | Acc: 0.793\n",
      "Epoch 008: | Loss: 0.44221 | Acc: 0.795\n",
      "Epoch 009: | Loss: 0.45634 | Acc: 0.798\n",
      "Epoch 010: | Loss: 0.46809 | Acc: 0.798\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "EPOCHS=10\n",
    "BATCH_SIZE=16\n",
    "num_subclasses = 3\n",
    "if dataset=='fetal':\n",
    "  EPOCHS=EPOCHS_FETAL\n",
    "\n",
    "ids = np.concatenate((easy_train, ambig_train))\n",
    "\n",
    "train_data = TrainDataDRO(torch.FloatTensor(X_train[ids,:]), \n",
    "                       torch.FloatTensor(y_train[ids]), \n",
    "                       torch.tensor(subclass_labels[ids]))\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dro_net = Net1(X_train[ids,:].shape[1],nlabels=nlabels)\n",
    "dro_net.to(device)\n",
    "\n",
    "optimizer = optim.Adam(dro_net.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "robust=True\n",
    "\n",
    "if robust:\n",
    "    size_adjustment=0\n",
    "    size_adjustments = [size_adjustment] * num_subclasses\n",
    "else:\n",
    "    size_adjustments = None\n",
    "robust_lr=0.001\n",
    "criterion = LossComputer(criterion=criterion, is_robust=robust, n_groups = num_subclasses, group_counts=subclass_counts,\n",
    "                          robust_step_size= robust_lr, stable=True,\n",
    "                          size_adjustments=size_adjustments,\n",
    "                          auroc_version=False,\n",
    "                          class_map=sup_sub_map, use_cuda=True)\n",
    "\n",
    "dro_net = train_loop(net=dro_net, criterion=criterion, EPOCHS=EPOCHS, train_loader=train_loader,optimizer=optimizer, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNO5j1tr_3Ve"
   },
   "source": [
    "# JTT based on errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LX8aoPLSV2wX",
    "outputId": "498b003f-5703-411d-a570-bfceaf63804e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/src/utils/group_dro_helpers.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = criterion(sf(y_pred), y_batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.68446 | Acc: 0.574\n",
      "Epoch 002: | Loss: 0.68309 | Acc: 0.575\n",
      "Epoch 003: | Loss: 0.68064 | Acc: 0.574\n",
      "Epoch 004: | Loss: 0.66707 | Acc: 0.602\n",
      "Epoch 005: | Loss: 0.64115 | Acc: 0.644\n",
      "Epoch 006: | Loss: 0.62301 | Acc: 0.671\n",
      "Epoch 007: | Loss: 0.61618 | Acc: 0.677\n",
      "Epoch 008: | Loss: 0.60513 | Acc: 0.701\n",
      "Epoch 009: | Loss: 0.60129 | Acc: 0.701\n",
      "Epoch 010: | Loss: 0.59852 | Acc: 0.708\n"
     ]
    }
   ],
   "source": [
    "n_reps=5\n",
    "\n",
    "# Get errors on base model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net_test = checkpoint_list[0][9] \n",
    "net_test.eval()\n",
    "with torch.no_grad():\n",
    "    X_batch = torch.tensor(X_train)\n",
    "    X_batch = X_batch.to(device)\n",
    "    y_test_pred = net_test(X_batch)\n",
    "threshold=0.5\n",
    "preds = y_test_pred.data[:,1].cpu().numpy()\n",
    "y_pred = preds>threshold\n",
    "\n",
    "# Augment with the errors\n",
    "aug_ids = np.where(np.not_equal(y_pred, y_train))[0]\n",
    "aug_feats = np.repeat(X_train[aug_ids,:], n_reps, axis=0)\n",
    "aug_labels = np.repeat(y_train[aug_ids], n_reps, axis=0)\n",
    "\n",
    "# Update the training set\n",
    "X_train_jtt = np.vstack((X_train, aug_feats))\n",
    "y_train_jtt = np.hstack((y_train, aug_labels))\n",
    "\n",
    "\n",
    "# RE-TRAIN\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS=10\n",
    "BATCH_SIZE=128\n",
    "\n",
    "if dataset=='fetal':\n",
    "  EPOCHS=EPOCHS_FETAL\n",
    "\n",
    "jtt_net = Net1(X_train.shape[1],nlabels=nlabels)\n",
    "jtt_net.to(device)\n",
    "\n",
    "train_data = TrainData(torch.FloatTensor(X_train_jtt), \n",
    "                       torch.FloatTensor(y_train_jtt))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = optim.Adam(jtt_net.parameters(), lr=LEARNING_RATE)\n",
    "jtt_net = train_loop(net=jtt_net, criterion=criterion, EPOCHS=EPOCHS, train_loader=train_loader,optimizer=optimizer, device=device, subclass=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJl20xQAfcIP",
    "outputId": "2c1754d6-d143-4ac1-a59b-22562d956659"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/src/models/neuralnets.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n",
      "/content/src/models/neuralnets.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n",
      "/content/src/models/neuralnets.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n",
      "/content/src/models/neuralnets.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n",
      "/content/src/models/neuralnets.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n",
      "/content/src/models/neuralnets.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n",
      "/content/src/models/neuralnets.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n",
      "/content/src/models/neuralnets.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n",
      "/content/src/models/neuralnets.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n",
      "/content/src/models/neuralnets.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n",
      "/content/src/models/neuralnets.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = F.softmax(self.output(X))\n"
     ]
    }
   ],
   "source": [
    "results={}\n",
    "results['jtt'] = evaluate_model(net_test=jtt_net, X_test=X_test, y_test=y_test, easy_test=easy_test, incons_test=ambig_test, hard_test=hard_test)\n",
    "\n",
    "results['dro_iq'] = evaluate_model(net_test=dro_net, X_test=X_test, y_test=y_test, easy_test=easy_test, incons_test=ambig_test, hard_test=hard_test)\n",
    "                                   \n",
    "results['dro-george'] = evaluate_model(net_test=dro_netG, X_test=X_test, y_test=y_test, easy_test=easy_test, incons_test=ambig_test, hard_test=hard_test)\n",
    "\n",
    "results['baseline'] = evaluate_model(net_test=checkpoint_list[0][9], X_test=X_test, y_test=y_test, easy_test=easy_test, incons_test=ambig_test, hard_test=hard_test)\n",
    "\n",
    "# ADDITIONAL METHOD - CAN UNCOMMENT --> this is group-dro on the clustered space of Data-IQ subgroups\n",
    "#results['dro-cluster'] = evaluate_model(net_test= dro_netC, X_test=X_test, y_test=y_test, easy_test=easy_test, incons_test=ambig_test, hard_test=hard_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWWeYILsCjOl",
    "outputId": "5466d40a-61b9-47b0-9510-dbd99f37d27b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jtt': {'overall': 0.40864511442063206,\n",
       "  'rest': 0.44543249797898143,\n",
       "  'ambig': 0.3806179775280899},\n",
       " 'dro_iq': {'overall': 0.7232110424990918,\n",
       "  'rest': 0.8302344381568311,\n",
       "  'ambig': 0.6306179775280899},\n",
       " 'dro-george': {'overall': 0.6785325099891028,\n",
       "  'rest': 0.7865804365400162,\n",
       "  'ambig': 0.5884831460674157},\n",
       " 'baseline': {'overall': 0.7221213221939702,\n",
       "  'rest': 0.8302344381568311,\n",
       "  'ambig': 0.6285112359550562}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dataiq_env_test",
   "language": "python",
   "name": "dataiq_env_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
